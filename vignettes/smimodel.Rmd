---
title: "Introduction to smimodel"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to smimodel}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---



## What is smimodel?

The *smimodel* package provides functions to estimate **Sparse Multiple Index (SMI) Models** for nonparametric forecasting or prediction. To support time series forecasting, the package functions are mainly build upon tidy temporal data in the `tsibble` format. However, the SMI model formulation is very general and does not exclusively depend on any temporal features. Hence, the model can be used more widely -- even with non-temporal cross-sectional data. (In such case, a numeric `index` (instead of a date or time related variable) can be used when constructing the `tsibble`.)

The SMI Modelling algorithm (i.e. the estimation algorithm of SMI model) that we implement here, simultaneously performs automatic predictor selection ("sparse") and predictor grouping, which is especially useful in obtaining a parsimonious model in high-dimensional contexts. For detailed information regarding the SMI model and the estimation algorithm refer the related working paper at https://www.monash.edu/business/ebs/research/publications/ebs/2024/wp16-2024.pdf.

## How to use smimodel?

Here we present a simple example to illustrate *smimodel* functionalities. We use randomly simualted data, which we treat as time series data for the purpose of this illustration. 

(Note: Since the SMI model estimation algorithm works with very limited amount of prior information, and handles automatic predictor selection and predictor grouping, the computational time for model estimation increases as the number of predictors and the number of indices increase. Therefore, we use a small simulated data set here as the example to reduce computational cost.)

### Data simulation

Suppose we are interested in forecasting a response variable $y$, which is an additive function of two nonlinear components involving five predictor variables $x\_000, x\_001, x\_002, x\_003$ and $x\_005$ plus a normally distributed white noise component. Here, the variables $x\_001, x\_002, x\_003$ and $x\_005$ correspond to the first, second, third and fifth lags of $x\_000$ respectively. 


``` r
## Load required packages 
library(smimodel)
library(dplyr)
library(ROI)
library(tibble)
library(tidyr)
library(tsibble)
```


``` r
## Simulate data

# Length of the time series
n = 1405 

# Set a seed for reproduciblity
set.seed(123)

# Generate data
sim_data <- tibble(x_000 = runif(n)) %>%
  mutate(
    # Add x_lags
    x = lag_matrix(x_000, 5)) %>%
  unpack(x, names_sep = "_") %>%
  mutate(
    # Response variable
    y = (0.9*x_000 + 0.6*x_001 + 0.45*x_003)^3 + 
      (0.35*x_002 + 0.7*x_005)^2 + rnorm(n, sd = 0.1),
    # Add an index to the data set
    inddd = seq(1, n)) %>%
  drop_na() %>%
  select(inddd, y, starts_with("x_")) %>%
  # Make the data set a `tsibble`
  as_tsibble(index = inddd)
```

Note that here we create an additional `numeric` variable `inddd` to serve as the `index` of the data set, when we convert the data set into an object of class `tsibble`. 

Next, we split the data into training and test sets so that the models can be estimated using the training data set, and the fitted models can be evaluated on the predictions obtained for the test set, which is not used for model estimation. 


``` r
## Data Split

# Training set
sim_train <- sim_data[1:1200, ]

# Test set
sim_test <- sim_data[1201:1400, ]

# Here, we sequentially split the data as we assume time series data. 
```

### SMI model estimation

We first train SMI models on the training set using three different initialisation options "ppr", "additive" and "linear" for comparison purposes. Please refer to package documentation/working paper for more information regarding available initialisation options.

(Note: The choice of the initialisation largely depends on the data and application. Thus, users are encouraged to follow a trial-and-error procedure to determine the most suitable initial model for a given application.)

Here, we assume that we do not have any prior knowledge about the construction of the response variable $y$. Hence, we input $x\_000$ and its first five lags as our predictor variables into the estimation algorithm, as predictors which are entering indices. 


``` r
## Index variables
index.vars <- colnames(sim_data)[3:8]
```

#### SMI model with "ppr" initialisation:

``` r
## SMI model with "PPR" initialisation
smimodel_ppr <- model_smimodel(data = sim_train,
                               yvar = "y",
                               index.vars = index.vars,
                               initialise = "ppr")
#> [1] "model 1"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
```


``` r
# Fitted optimal SMI model
smimodel_ppr$fit[[1]]$best
#> $alpha
#> 6 x 2 sparse Matrix of class "dgCMatrix"
#>          index1   index2
#> x_000 1.6279857 .       
#> x_001 1.0728102 .       
#> x_002 .         1.211800
#> x_003 0.8107054 .       
#> x_004 .         .       
#> x_005 .         2.298439
#> 
#> $derivatives
#> # A tibble: 1,200 × 2
#>       d1    d2
#>    <dbl> <dbl>
#>  1 0.973 0.387
#>  2 1.41  0.550
#>  3 4.06  0.190
#>  4 1.74  0.460
#>  5 1.62  0.603
#>  6 4.04  0.159
#>  7 2.60  0.390
#>  8 1.90  0.599
#>  9 2.89  0.387
#> 10 0.777 0.375
#> # ℹ 1,190 more rows
#> 
#> $var_y
#> [1] "y"
#> 
#> $vars_index
#> [1] "x_000" "x_001" "x_002" "x_003" "x_004" "x_005"
#> 
#> $vars_s
#> NULL
#> 
#> $vars_linear
#> NULL
#> 
#> $neighbour
#> [1] 0
#> 
#> $gam
#> 
#> Family: gaussian 
#> Link function: identity 
#> 
#> Formula:
#> y ~ s(index1, bs = "cr") + s(index2, bs = "cr")
#> 
#> Estimated degrees of freedom:
#> 8.5 6.1  total = 15.6 
#> 
#> REML score: -1028.854     
#> 
#> attr(,"class")
#> [1] "smimodelFit" "list"
```


``` r
# Estimated index structure
smimodel_ppr$fit[[1]]$best$alpha
#> 6 x 2 sparse Matrix of class "dgCMatrix"
#>          index1   index2
#> x_000 1.6279857 .       
#> x_001 1.0728102 .       
#> x_002 .         1.211800
#> x_003 0.8107054 .       
#> x_004 .         .       
#> x_005 .         2.298439
```

#### SMI model with "additive" initialisation:

``` r
## SMI model with "Additive" initialisation
smimodel_additive <- model_smimodel(data = sim_train,
                                    yvar = "y",
                                    index.vars = index.vars,
                                    initialise = "additive")
#> [1] "model 1"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
```


``` r
# Fitted optimal SMI model
smimodel_additive$fit[[1]]$best
#> $alpha
#> 6 x 2 sparse Matrix of class "dgCMatrix"
#>          index1    index2
#> x_000 0.4634955 .        
#> x_001 0.3054954 .        
#> x_002 .         0.3451773
#> x_003 0.2310091 .        
#> x_004 .         .        
#> x_005 .         0.6548227
#> 
#> $derivatives
#> # A tibble: 1,200 × 2
#>       d1    d2
#>    <dbl> <dbl>
#>  1  3.42 1.36 
#>  2  4.95 1.93 
#>  3 14.3  0.667
#>  4  6.12 1.62 
#>  5  5.69 2.12 
#>  6 14.2  0.559
#>  7  9.14 1.37 
#>  8  6.69 2.10 
#>  9 10.1  1.36 
#> 10  2.73 1.31 
#> # ℹ 1,190 more rows
#> 
#> $var_y
#> [1] "y"
#> 
#> $vars_index
#> [1] "x_000" "x_001" "x_002" "x_003" "x_004" "x_005"
#> 
#> $vars_s
#> NULL
#> 
#> $vars_linear
#> NULL
#> 
#> $neighbour
#> [1] 0
#> 
#> $gam
#> 
#> Family: gaussian 
#> Link function: identity 
#> 
#> Formula:
#> y ~ s(index1, bs = "cr") + s(index2, bs = "cr")
#> 
#> Estimated degrees of freedom:
#> 8.50 6.09  total = 15.59 
#> 
#> REML score: -1028.856     
#> 
#> attr(,"class")
#> [1] "smimodelFit" "list"
```


``` r
# Estimated index structure
smimodel_additive$fit[[1]]$best$alpha
#> 6 x 2 sparse Matrix of class "dgCMatrix"
#>          index1    index2
#> x_000 0.4634955 .        
#> x_001 0.3054954 .        
#> x_002 .         0.3451773
#> x_003 0.2310091 .        
#> x_004 .         .        
#> x_005 .         0.6548227
```

#### SMI model with "linear" initialisation:

``` r
## SMI model with "Linear" initialisation
smimodel_linear <- model_smimodel(data = sim_train,
                                  yvar = "y",
                                  index.vars = index.vars,
                                  initialise = "linear")
#> [1] "model 1"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
```


``` r
# Fitted optimal SMI model
smimodel_linear$fit[[1]]$best
#> $alpha
#> 6 x 1 sparse Matrix of class "dgCMatrix"
#>           index1
#> x_000 0.40811741
#> x_001 0.26853838
#> x_002 0.04185720
#> x_003 0.20411136
#> x_004 .         
#> x_005 0.07737565
#> 
#> $derivatives
#> # A tibble: 1,200 × 1
#>       d1
#>    <dbl>
#>  1  4.35
#>  2  6.55
#>  3 14.4 
#>  4  8.30
#>  5  7.70
#>  6 14.0 
#>  7 10.3 
#>  8  9.97
#>  9 11.8 
#> 10  3.27
#> # ℹ 1,190 more rows
#> 
#> $var_y
#> [1] "y"
#> 
#> $vars_index
#> [1] "x_000" "x_001" "x_002" "x_003" "x_004" "x_005"
#> 
#> $vars_s
#> NULL
#> 
#> $vars_linear
#> NULL
#> 
#> $neighbour
#> [1] 0
#> 
#> $gam
#> 
#> Family: gaussian 
#> Link function: identity 
#> 
#> Formula:
#> y ~ s(index1, bs = "cr")
#> 
#> Estimated degrees of freedom:
#> 7.8  total = 8.8 
#> 
#> REML score: -445.1356     
#> 
#> attr(,"class")
#> [1] "smimodelFit" "list"
```


``` r
# Estimated index structure
smimodel_linear$fit[[1]]$best$alpha
#> 6 x 1 sparse Matrix of class "dgCMatrix"
#>           index1
#> x_000 0.40811741
#> x_001 0.26853838
#> x_002 0.04185720
#> x_003 0.20411136
#> x_004 .         
#> x_005 0.07737565
```


In this case, the SMI models fitted with "ppr" and "additive" initialisation options have correctly identified the index structure of the response $y$ by estimating two linear combinations (i.e. indices), while dropping out the irrelevant predictor $x\_004$. While correctly identifying $x\_004$ as an irrelevant predictor variable, the SMI model estimated with "linear" intialisation however, has not correctly identified the index structure of the model -- has estimated only a single index instead of the two indices. 

Thus, as mentioned earlier, the final estimated model can change depending on the initialisation option chosen. Hence, the users are encouraged to experiment with different available initialisation options when choosing the best fit for the data/application of interest. (In a real-world problem (where the true model is unknown), it will be useful to fit SMI models with different initialisation options, and see which option gives the best forecasting/prediction accuracy.)

### Visualisation of estimated smooths

Once you fit a SMI model, the partial effects of the estimated smooths corresponding to the estimated indices can be plotted using the `autoplot` method as below. 


``` r
## Plot estimated smooths of the SMI model with "ppr" initialisation
autoplot(object = smimodel_ppr)
```

![plot of chunk autoplot](figure/autoplot-1.png)

### Residuals and fitted values

We can use the `augment` method to obtain the residuals and the fitted values from an estimated SMI model.


``` r
## Obtain residuals and fitted values
augment(x = smimodel_ppr)
#> # A tibble: 1,200 × 3
#>    Index   .resid .fitted
#>    <int>    <dbl>   <dbl>
#>  1     6  0.0566    0.757
#>  2     7 -0.0397    1.50 
#>  3     8 -0.0552    3.78 
#>  4     9  0.0499    1.78 
#>  5    10  0.155     1.88 
#>  6    11 -0.145     3.70 
#>  7    12  0.200     2.16 
#>  8    13 -0.00571   2.20 
#>  9    14 -0.0585    2.78 
#> 10    15  0.0690    0.588
#> # ℹ 1,190 more rows
```

### Forecasts on a test set

Based on an estimated SMI model, we obtain forecasts/predictions on a test set as below, using the `predict` method.


``` r
## Obtain forecasts on the test set
preds = predict(object = smimodel_ppr, newdata = sim_test)
#> Error in meanf(object, h = h, level = level, fan = fan, lambda = lambda, : unused argument (newdata = list(1206:1405, c(3.49124169545011, 1.48362868456093, 0.502781151466303, 2.27193619962515, 1.14531957808155, 1.45610984085034, 3.44879723658308, 1.21856507692158, 1.21510637841818, 0.777318678622333, 1.13879005345479, 1.55195753203221, 1.19285369023581, 1.26592393873102, 1.53103764698884, 2.37005214231825, 0.78447230639341, 1.19430266360755, 1.08636567330945, 4.61157154321595, 0.470003649662609, 1.01924959240427, 0.883525899157584, 1.43646535084252, 2.41344470602436, 0.210141931929313, 
#> 0.637251369935488, 0.48483017070957, 0.967151385758648, 0.540051693859614, -0.00591690534489676, 0.192829269886108, 0.687094548337608, 2.23704857017094, 0.972787800623502, 0.248468455796017, 0.446560560518275, 0.937642573921337, 1.07642135846068, 1.36846596926693, 1.1095286857625, 0.842464729814123, 0.630526633304537, 0.181361282139659, 1.39781262875024, 1.76451132929887, 1.65183991960776, 1.67212413073567, 0.510996840980813, 0.340838317973449, 1.7758057963399, 0.182496225707728, 0.765780887805571, 
#> 2.87258591205838, 1.41653317072242, 1.21698679549925, 1.58445535932184, 0.456804606298418, 0.948892290643738, 0.782124436629586, 0.269516644409916, 0.564125078993966, 3.67479247159151, 0.249428593046589, 2.41414442388989, 1.1151045093036, 1.18091880331075, 2.48594504621317, 0.8366968619727, 3.62113303953107, 1.85194307899537, 2.55369432217125, 2.88924298012293, 1.21559903430533, 1.39883053317104, 0.357327249260404, 1.41962608127244, 1.96885399110918, 1.85271563305469, 2.74894999933475, 0.848028509082333, 
#> 2.4123314000411, 1.95653420413829, 1.09935402928691, 1.24013561056973, 2.45910901240064, 1.86299612209842, 0.779584087472622, 2.09700154359553, 1.66581902624261, 0.849626097247215, 2.28158752445974, 2.18492647368283, 3.20300505719549, 2.88106799282498, 0.484330876375508, 0.879319320585379, 1.57017242078446, 2.08452300185845, 2.88952363358089, 1.90686097300448, 0.883656221683908, 0.427785135368411, 1.29809079449427, 0.462613933115877, 0.49111505664609, 0.472619554531524, 1.15215841506567, 0.889414767405721, 
#> 0.997940424282521, 0.924419182096793, 0.992723113163425, 1.48599353487017, 0.552356378895732, 0.696160627938453, 0.56078848349202, 1.32881657641147, 1.44532143106875, 2.15143239251275, 1.99329991511979, 2.9653679860765, 0.806993063267065, 2.2397432320482, 0.203333035586841, 1.1885383715076, 1.34081109537064, 0.478576008768312, 1.27662766619159, 1.5084639051408, 1.06419969381383, 0.879954928978607, 1.3197077558515, 2.44087718061084, 2.40808377207946, 2.66875800605847, 1.09906425009879, 1.44237266345647, 
#> 1.08049007302439, 0.71776262976184, 0.934254863606583, 3.00845994344627, 1.33170081065305, 1.55531353894963, 2.17088609304485, 1.74388974176988, 1.00039187818363, 0.244443383064422, 0.363092362386087, 0.108011716088827, 0.241389845688585, 0.123337892716965, 0.111588462436474, 0.899819657518003, 0.888271389531245, 0.385985761505963, 2.2435527237008, 0.441588283874196, 1.67982534140622, 1.71940589840161, 0.713374843791116, 0.538975431559374, 0.123878832171816, 0.957385207634123, 0.415557958390278, 
#> 0.0796062394100005, 0.431013040324381, 0.038154833257998, 0.346458009662973, 0.660795831115044, 1.37469317700219, 2.60824644866285, 1.51871231915771, 3.17827515485932, 1.07891489715191, 0.997401093188051, 2.04560480378541, 0.437245201396994, 3.02189977021293, 0.653553937464913, 1.45339620156137, 1.50243841013214, 0.241936790268776, 2.61481425058869, 0.369841510025043, 0.998178525980495, 2.89159786476145, 1.7430900805267, 1.7096876262859, 1.26646771472005, 1.02338041495689, 2.23273213146023, 2.46246871305876, 
#> 1.44666530331206, 0.406412247646625, 0.397387842614919, 0.525586144889597, 0.449129836713747, 0.0306210066043653, 0.312672444538681, 0.276866814666007), c(0.988708860939369, 0.0647511295974255, 0.157662777928635, 0.785348618403077, 0.542188289808109, 0.416547345230356, 0.998882649000734, 0.255673117004335, 0.507875671144575, 0.0789711775723845, 0.813668364426121, 0.38217916013673, 0.802182707935572, 0.197923636762425, 0.946399770211428, 0.345672474242747, 0.522050247294828, 0.111688006436452, 0.885974851204082, 
#> 0.95425497321412, 0.0403807631228119, 0.4936161886435, 0.226063704816625, 0.858800339279696, 0.53093573381193, 0.0046381508000195, 0.277560080168769, 0.325203143060207, 0.588706276612356, 0.249684700742364, 0.0431172808166593, 0.110678787576035, 0.703753812005743, 0.939021238824353, 0.311169018037617, 0.0784929301589727, 0.321744091343135, 0.624905536882579, 0.440241849515587, 0.801345300627872, 0.279283805051818, 0.570713193388656, 0.0421280120499432, 0.190717455465347, 0.727086470695212, 0.826690050307661, 
#> 0.510721075348556, 0.567726165754721, 0.00115581974387169, 0.143778103170916, 0.865967083023861, 0.0825610605534166, 0.244570682058111, 0.98154315748252, 0.57758127944544, 0.24872636445798, 0.614953953772783, 0.0317573207430542, 0.146423544269055, 0.703072973992676, 0.065560708520934, 0.621807062299922, 0.937327365390956, 0.0878946147859097, 0.972803509328514, 0.0260107105132192, 0.725227952003479, 0.453706391621381, 0.566182404989377, 0.87273104628548, 0.55456317961216, 0.737652207259089, 0.62312916200608, 
#> 0.311028405791149, 0.392196484608576, 0.202087455429137, 0.852780389599502, 0.606830150820315, 0.75609801383689, 0.562516808742657, 0.275899012805894, 0.735558403655887, 0.549279793864116, 0.346127043943852, 0.514965634793043, 0.815767947584391, 0.526272569317371, 0.203040049877018, 0.848107680445537, 0.370494215050712, 0.303325170883909, 0.770597046939656, 0.733448554994538, 0.838905998272821, 0.569048842415214, 0.02628083829768, 0.486127228243276, 0.542681032791734, 0.836660116910934, 0.728664538124576, 
#> 0.555113806854934, 0.0734143974259496, 0.136502299224958, 0.684608696261421, 0.0996219362132251, 0.203785905148834, 0.365980138303712, 0.846195757621899, 0.236872595502064, 0.704877850599587, 0.107608830789104, 0.779068868607283, 0.25510568334721, 0.548043326009065, 0.0339015207719058, 0.806109476136044, 0.337752807186916, 0.916285858023912, 0.357106417883188, 0.975365013116971, 0.384378522634506, 0.488673535175622, 0.490776231279597, 0.0122546241618693, 0.641473200637847, 0.512574971653521, 0.320352118229493, 
#> 0.58143315766938, 0.617344408296049, 0.438274537678808, 0.170368980616331, 0.71508049662225, 0.747786548687145, 0.843914708122611, 0.534759046975523, 0.302235261537135, 0.456518182065338, 0.384342534467578, 0.249046854674816, 0.654626744799316, 0.95532370172441, 0.291082957293838, 0.650189086562023, 0.536359930876642, 0.597451866604388, 0.0248982694465667, 0.00484173954464495, 0.131378254387528, 0.0248171037528664, 0.0921294519212097, 0.475558224134147, 0.196656761225313, 0.814673948567361, 0.244059228105471, 
#> 0.488807637942955, 0.648882479639724, 0.158716470003128, 0.836977910948917, 0.436596529791132, 0.233495525084436, 0.298855992034078, 0.159421336837113, 0.585567104164511, 0.14877797709778, 0.179059559712186, 0.339221504982561, 0.18390193884261, 0.405541522195563, 0.613559618359432, 0.685671334387735, 0.823525600135326, 0.373982113786042, 0.952841004589573, 0.0188321589957923, 0.61237497231923, 0.463573763845488, 0.354729620506987, 0.882423028815538, 0.075802858453244, 0.88329119165428, 0.188221835764125, 
#> 0.215066746808589, 0.869532291544601, 0.0608380562625825, 0.234899600734934, 0.993015512824059, 0.678197727771476, 0.435486800502986, 0.37730994191952, 0.46118402434513, 0.801851871656254, 0.662966975243762, 0.46600366407074, 0.053259436506778, 0.23719791835174, 0.233239737572148, 0.230653600767255, 0.0617262739688158, 0.497118609258905, 0.24412508867681), c(0.486705572577193, 0.988708860939369, 0.0647511295974255, 0.157662777928635, 0.785348618403077, 0.542188289808109, 0.416547345230356, 0.998882649000734, 
#> 0.255673117004335, 0.507875671144575, 0.0789711775723845, 0.813668364426121, 0.38217916013673, 0.802182707935572, 0.197923636762425, 0.946399770211428, 0.345672474242747, 0.522050247294828, 0.111688006436452, 0.885974851204082, 0.95425497321412, 0.0403807631228119, 0.4936161886435, 0.226063704816625, 0.858800339279696, 0.53093573381193, 0.0046381508000195, 0.277560080168769, 0.325203143060207, 0.588706276612356, 0.249684700742364, 0.0431172808166593, 0.110678787576035, 0.703753812005743, 0.939
preds$.predict
#> Error: object 'preds' not found
```

Once we obtain forecasts/predictions, we can evaluate the forecasting/predictive performance of the estimated SMI model by calculating forecast/prediction error measurements as desired.


``` r
## Calculate test set MSE and MAE
MSE_SMI_ppr = MSE(residuals = (preds$y - preds$.predict))
#> Error: object 'preds' not found
MSE_SMI_ppr
#> Error: object 'MSE_SMI_ppr' not found
MAE_SMI_ppr = MAE(residuals = (preds$y - preds$.predict))
#> Error: object 'preds' not found
MAE_SMI_ppr
#> Error: object 'MAE_SMI_ppr' not found
```

### Tuning for penalty parameters

The estimation of a SMI model involves solving an optimaisation problem, where the sum of squared errors of the model plus two penalty terms (an L0 penalty and an L2 (ridge) penalty) is minimised subject to a set of constraints (please refer the working paper for details). Thus, two penalty parameters $\lambda_{0}$ and $\lambda_{2}$ corresponding to the L0 and L2 penalties respectively should be chosen when estimating a SMI model. 

In the previous example, all the SMI models were fitted using the default penalty parameter values provided in the function: $\lambda_{0} = 1$ and $\lambda_{2} = 1$. To fit a SMI model with simultaneous parameter tuning, we can use the function `greedy_smimodel()`, which performs a greedy search over a given grid of penalty parameter combinations ($\lambda_{0}$, $\lambda_{2}$), and fits the SMI model using the best (lowest validation set MSE) penalty parameter combination. In this case, we need to provide a validation set, which is separate from the training data set. 

Therefore, let's split our original training set in the above example into two parts again to obtain a validation set. 


``` r
# New training set
sim_train_new <- sim_data[1:1000, ]

# Validation set
sim_val_new <- sim_data[1001:1200, ]
```

Next, we can estimate the SMI model with simultaneous penalty parameter tuning as follows. Here, we use the initialisation option "ppr" just to demonstrate the functionality. 


``` r
## Estimating SMI model with penalty parameter tuning
smimodel_ppr_tune <- greedy_smimodel(data = sim_train_new, 
                                     val.data = sim_val_new,
                                     yvar = "y",
                                     index.vars = index.vars,
                                     initialise = "ppr")
#> [1] "model 1"
#> [1] "model 1"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Potential starting points completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 1 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 2 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 3 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Maximum iterations reached!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 4 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 5 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 6 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 7 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 8 completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Initial search around potential starting point 9 completed."
#> [1] "Starting point for the greedy search selected."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "An iteration of greedy search - step 1 is completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "An iteration of greedy search - step 1 is completed."
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "Tolerance for loss reached!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Loss increased for 3 consecutive iterations!"
#> [1] "Final model fitted!"
#> [1] "An iteration of greedy search - step 1 is completed."
#> [1] "Maximum iterations reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Tolerance for loss reached!"
#> [1] "Final model fitted!"
#> [1] "Final SMI model is fitted."
```


``` r
# Fitted optimal SMI model
smimodel_ppr_tune$fit[[1]]$best
#> $alpha
#> 6 x 2 sparse Matrix of class "dgCMatrix"
#>          index1   index2
#> x_000 1.6282120 .       
#> x_001 1.0726790 .       
#> x_002 .         1.208052
#> x_003 0.8106106 .       
#> x_004 .         .       
#> x_005 .         2.302190
#> 
#> $derivatives
#> # A tibble: 1,200 × 2
#>       d1    d2
#>    <dbl> <dbl>
#>  1 0.973 0.385
#>  2 1.41  0.550
#>  3 4.06  0.190
#>  4 1.74  0.461
#>  5 1.62  0.603
#>  6 4.04  0.159
#>  7 2.60  0.389
#>  8 1.90  0.599
#>  9 2.89  0.386
#> 10 0.777 0.374
#> # ℹ 1,190 more rows
#> 
#> $var_y
#> [1] "y"
#> 
#> $vars_index
#> [1] "x_000" "x_001" "x_002" "x_003" "x_004" "x_005"
#> 
#> $vars_s
#> NULL
#> 
#> $vars_linear
#> NULL
#> 
#> $neighbour
#> [1] 0
#> 
#> $gam
#> 
#> Family: gaussian 
#> Link function: identity 
#> 
#> Formula:
#> y ~ s(index1, bs = "cr") + s(index2, bs = "cr")
#> 
#> Estimated degrees of freedom:
#> 8.50 6.08  total = 15.58 
#> 
#> REML score: -1028.858     
#> 
#> attr(,"class")
#> [1] "smimodelFit" "list"
```


``` r
## Selected penalty parameter combination
smimodel_ppr_tune$fit[[1]]$best_lambdas
#> [1] 0.01489049 0.01000000
```

Here the selected penalty parameter combination is $(\lambda_{0}, \lambda_{2}) = (0.01489049, 0.01000000)$.
